import streamlit as st
from streamlit_option_menu import option_menu
from streamlit_card import card
from streamlit_lottie import st_lottie  # pip install streamlit-lottie
import pandas as pd
import yaml, json, requests, base64
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
import os, zipfile
from PIL import Image
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler

def scaling(df, choose):
        if choose == "Min-Max Scaling":
                # Min-Max Scaling
                min_max_scaler = MinMaxScaler()
                df_min_max_scaled = pd.DataFrame(min_max_scaler.fit_transform(df), columns=df.columns)
                return df_min_max_scaled
        elif choose == "Z-Score Scaling":
                # Z-Score Scaling
                standard_scaler = StandardScaler()
                df_standard_scaled = pd.DataFrame(standard_scaler.fit_transform(df), columns=df.columns)
                return df_standard_scaled
        elif choose == "Robust Scaling":
                # Robust Scaling
                robust_scaler = RobustScaler()
                df_robust_scaled = pd.DataFrame(robust_scaler.fit_transform(df), columns=df.columns)
                return df_robust_scaled
        elif choose == "MaxAbs Scaling":
                # MaxAbs Scaling
                maxabs_scaler = MaxAbsScaler()
                df_maxabs_scaled = pd.DataFrame(maxabs_scaler.fit_transform(df), columns=df.columns)
                return df_maxabs_scaled
        else:
               st.write("GeÃ§ersiz seÃ§im")
               return df



#Animasyon ekleme
def load_lottiefile(filename):
    with open(filename, 'r', encoding='utf-8') as f:
        return json.load(f)

#Loading animasyonu
lottie_coding = load_lottiefile("media/lottieFiles/loading.json")  # replace link to local lottie file

#Temizlenen dosyasÄ± indirme
def convert_df(df):
    # IMPORTANT: Cache the conversion to prevent computation on every rerun
    return df.to_csv().encode("utf-8")

def save_uploaded_file(uploadedfile):
    with open(os.path.join("tempDir", uploadedfile.name),"wb") as f:
        f.write(uploadedfile.getbuffer())
    return st.success("Saved file :{} in tempDir".format(uploadedfile.name))

#Sekme 1 fonksiyonu: Ä°lk ve son 10 satÄ±rÄ± gÃ¶sterir.
def row_of_datasets(df):
        st.write("**First 10 Rows**")
        st.write(df.head(10))  # Ä°lk 10 satÄ±rÄ± gÃ¶sterme
        st.write("**Last 10 Rows**")
        st.write(df.tail(10))  # Son 10 satÄ±rÄ± gÃ¶sterme

#Sekme 2: SayÄ±sal ve genel bilgileri tablolar aracÄ±lÄ±ÄŸÄ±yla gÃ¶sterir.
def information(df):
         # Bilgi aÃ§Ä±klamasÄ±
        st.caption("The table below shows the number of rows, columns, empty columns, and column names of the dataset.")

        # SatÄ±r ve sÃ¼tun sayÄ±sÄ±, sÃ¼tun isimleri ve boÅŸ hÃ¼cre sayÄ±sÄ±nÄ± gÃ¶sterir
        data = pd.DataFrame({
                "Information": ["Row", "Columns", "Column Names", "Empty Values"],
                "Value": [df.shape[0], df.shape[1], df.columns.tolist(), df.isnull().sum().sum()]
        })
        st.dataframe(data)

        # SÃ¼tunlarÄ±n tÃ¼rlerini gÃ¶rme ve beÅŸ satÄ±r Ã¶rnek gÃ¶sterme
        col1, col2 = st.columns([0.4, 0.6])
        with col1:
                st.write("**Data Types of Columns**")
                st.write(df.dtypes)
        with col2:
                st.write("**From which column would you like to see sample values?**")
                selected_column = st.selectbox("Column Names:", df.columns.tolist())
                st.write(df[selected_column].head())
        
        # Kategorik sÃ¼tunlarÄ±n istatistikleri
        if any(df.dtypes == 'object'):
                st.write("**Statistics of Categorical Columns**")
                st.write(df.describe(include=['O']))
        else:
                st.info('No categorical data available.', icon="â„¹ï¸")

        # SayÄ±sal sÃ¼tunlarÄ±n istatistikleri
        if any(df.dtypes != 'object'):
                st.write("**Statistics of Numerical Columns**")
                st.write(df.describe())
        else:
                st.info('No numerical data available.', icon="â„¹ï¸")

def save_uploaded_file(uploadedfile):
    with open(os.path.join("tempDir", uploadedfile.name), "wb") as f:
        f.write(uploadedfile.getbuffer())
    return st.success(f"Saved file: {uploadedfile.name} in tempDir")

def analyze_image_folders(base_dir):
    folder_data = []
    
    for root, dirs, files in os.walk(base_dir):
        folder_info = {
            "folder_name": os.path.basename(root),
            "file_count": len(files),
            "files": files,
            "file_types": {}
        }
        
        for file in files:
            file_extension = os.path.splitext(file)[1].lower()
            if file_extension in folder_info["file_types"]:
                folder_info["file_types"][file_extension] += 1
            else:
                folder_info["file_types"][file_extension] = 1
        
        folder_data.append(folder_info)
    
    return folder_data

def display_analysis(base_dir, folder_data):
    folder_info = []
    for folder in folder_data:
        folder_name = folder['folder_name']
        file_count = folder['file_count']
        files = ', '.join(folder['files'][:20]) if file_count > 20 else ', '.join(folder['files'])
        file_types = folder['file_types']

        folder_info.append([folder_name, file_count, files, file_types])

    df = pd.DataFrame(folder_info, columns=['KlasÃ¶r AdÄ±', 'Dosya SayÄ±sÄ±', 'Ä°lk 20 Dosya AdÄ±', 'Dosya TÃ¼rleri ve SayÄ±larÄ±'])

    # Ek analizler
    for folder in folder_data:
        folder_path = os.path.join(base_dir, folder["folder_name"])
        file_count = folder['file_count']
        if file_count > 20:
            st.write(f"--- {folder['folder_name']} ---")
            st.write(f"Ä°lk 20 dosya adÄ±: {', '.join(folder['files'][:20])}")
            st.write("Dosya tÃ¼rleri ve sayÄ±larÄ±:")
            for file_type, count in folder["file_types"].items():
                st.write(f"  - {file_type}: {count}")
            st.write(f"Toplam dosya sayÄ±sÄ±: {file_count}")
            st.write("Analizler:")

    st.table(df)

def display_sample_images(base_dir, folder_data):
        try:
                st.write("Ã–rnek GÃ¶rseller:")
                for folder in folder_data:
                        folder_path = os.path.join(base_dir, folder["folder_name"])
                        sample_files = folder["files"][:3]  # Ä°lk 3 dosyayÄ± seÃ§
                        for file in sample_files:
                                file_path = os.path.join(folder_path, file)
                                if is_valid_image_file(file_path):
                                        image = Image.open(file_path)
                                        st.image(image, caption=file)
        except (pd.errors.EmptyDataError, pd.errors.ParserError):
                st.error('YÃ¼klenen veri seti boÅŸ veya geÃ§ersiz.', icon="ğŸš¨")
        except Exception as e:
                st.error(f'Bir hata oluÅŸtu: {e}', icon="ğŸš¨")

def is_valid_image_file(file_path):
    valid_extensions = ['.jpg', '.jpeg', '.png']
    return any(file_path.lower().endswith(ext) for ext in valid_extensions)

def plot_image_statistics(folder_data):
    folder_names = [folder["folder_name"] for folder in folder_data]
    file_counts = [folder["file_count"] for folder in folder_data]
    
    # Bar grafiÄŸi
    plt.figure(figsize=(10, 6))
    plt.bar(folder_names, file_counts, color='skyblue')
    plt.xlabel('KlasÃ¶r AdÄ±')
    plt.ylabel('Dosya SayÄ±sÄ±')
    plt.title('KlasÃ¶rlerdeki Dosya SayÄ±sÄ±')
    plt.xticks(rotation=45)
    plt.tight_layout()
    st.pyplot(plt)

    # Pasta grafiÄŸi
    plt.figure(figsize=(8, 8))
    plt.pie(file_counts, labels=folder_names, autopct='%1.1f%%', startangle=140)
    plt.title('KlasÃ¶rlerdeki Dosya DaÄŸÄ±lÄ±mÄ±')
    plt.axis('equal')
    plt.tight_layout()
    st.pyplot(plt)

def prepare_data(df, operations, deleted, missing, normalize_columns, scaling):
    islemler = []
    st.write("YapÄ±lacak iÅŸlemler:", operations)
    newdata = df.copy()

    if "Remove Duplicate Rows" in operations: 
        newdata.drop_duplicates(inplace=True)
        islemler.append("Yinelenen satÄ±rlar silindi.")
    else:
        islemler.append("Yinelenen satÄ±r bulunmuyor. DeÄŸiÅŸiklik yapÄ±lmadÄ±.")

    if "Lowercase Column Names" in operations: 
        newdata.columns = [kolon.lower() for kolon in newdata.columns]
        islemler.append("SÃ¼tun isimleri kÃ¼Ã§Ã¼k harfe Ã§evrildi.")
    else:
        islemler.append("TÃ¼m sutÃ¼n isimleri kÃ¼Ã§Ã¼k harfle baÅŸlÄ±yor. DeÄŸiÅŸiklik yapÄ±lmadÄ±.")

    if "Lowercase Values in Object Data" in operations: 
        object_columns = newdata.select_dtypes(include=['object']).columns
        newdata[object_columns] = newdata[object_columns].apply(lambda x: x.astype(str).str.lower())
        islemler.append("Object veri tipinde olan sÃ¼tunlardaki deÄŸerler kÃ¼Ã§Ã¼k harfe dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼.")
    else:
        islemler.append("Object veri tipinde olan sÃ¼tunlardaki deÄŸerler kÃ¼Ã§Ã¼k harf. DeÄŸiÅŸiklik yapÄ±lmadÄ±.")

    if "Delete Unnecessary Columns" in operations and deleted: 
        newdata.drop(columns=deleted, inplace=True)
        islemler.append("Gereksiz kolonlar silindi.")
        
    if "Fill Missing Values with -Unknown- in String Values" in operations: 
        string_columns = newdata.select_dtypes(include=['object']).columns
        newdata[string_columns] = newdata[string_columns].fillna("Unknown")
        islemler.append("String deÄŸerlere 'Bilinmiyor' ile eksik deÄŸerler dolduruldu.")
    else:
        islemler.append("BoÅŸ bir String deÄŸer bulunmuyor. DeÄŸiÅŸiklik yapÄ±lmadÄ±.")


    if "Label Encoding" in operations: 
        label_encoder = LabelEncoder()
        string_columns = newdata.select_dtypes(include=['object']).columns
        newdata[string_columns] = newdata[string_columns].apply(label_encoder.fit_transform)
        islemler.append("Label Encoding iÅŸlemi tamamlandÄ±.")
           
    if "One-Hot Encoding" in operations:
        newdata = pd.get_dummies(newdata)
        islemler.append("One-Hot Encoding iÅŸlemi tamamlandÄ±.")
 
    if "Fill Missing Values in Selected Columns" in operations:
        missing_columns = newdata.columns[newdata.isnull().any()].tolist()
        if not missing_columns:
            st.write("Veri setinde eksik deÄŸer iÃ§eren sÃ¼tun bulunmuyor.")
        else:
            for column in missing:
                if newdata[column].isnull().any():
                    if newdata[column].dtype in ['int64', 'float64']:
                        imputer = SimpleImputer(strategy='mean')
                        newdata[[column]] = imputer.fit_transform(newdata[[column]])
                        st.write(f"{column} sÃ¼tunundaki eksik deÄŸerler ortalama ile dolduruldu.")

    if "Normalize" in operations:
                scaler = MinMaxScaler()
                df[normalize_columns] = scaler.fit_transform(df[normalize_columns])
                return df
                islemler.append("SeÃ§ilen sÃ¼tunlar normalize edildi.")
    else:
               pass

    if "Scaling" in operations:
                if scaling == "Min-Max Scaling":
                        # Min-Max Scaling
                        min_max_scaler = MinMaxScaler()
                        newdata = pd.DataFrame(min_max_scaler.fit_transform(df), columns=df.columns)
                elif scaling == "Z-Score Scaling":
                        # Z-Score Scaling
                        standard_scaler = StandardScaler()
                        newdata = pd.DataFrame(standard_scaler.fit_transform(df), columns=df.columns)
                elif scaling == "Robust Scaling":
                        # Robust Scaling
                        robust_scaler = RobustScaler()
                        newdata = pd.DataFrame(robust_scaler.fit_transform(df), columns=df.columns)
                elif scaling == "MaxAbs Scaling":
                        # MaxAbs Scaling
                        maxabs_scaler = MaxAbsScaler()
                        newdata = pd.DataFrame(maxabs_scaler.fit_transform(df), columns=df.columns)
                else:
                        st.write("GeÃ§ersiz seÃ§im")
                islemler.append("Scaling iÅŸlemi yapÄ±ldÄ±.")
    else:
               pass

    # Veri setini CSV formatÄ±nda indirme iÅŸlemi
    csv = convert_df(newdata)
    st.download_button(
        label="Download Data",
        data=csv,
        file_name="updatedDataset.csv",
        mime="text/csv",)
    
    st.write(islemler)

def auto_cleaning(df):
        op = ["Remove Duplicate Rows", "Lowercase Column Names", "Lowercase Values in Object Data" "Fill Missing Values with -Unknown- in String Values", "One-Hot Encoding"]
        prepare_data(df, op, [], [], [], [])

def myself_cleaning(df):
        with st.container(border = True):
                delete_columns = []
                missing_columns = []
                normalize_columns = []
                scaling_type = []
                op = ["Remove Duplicate Rows", #Yinelenen satÄ±rlarÄ± kaldÄ±rma 
                        "Lowercase Column Names",  #SÃ¼tun isimlerini kÃ¼Ã§Ã¼k harfe Ã§evirme
                        "Lowercase Values in Object Data," #Kategorik verileri kÃ¼Ã§Ã¼k harfe Ã§evirme
                        "Delete Unnecessary Columns", #Gereksiz kolonlarÄ± silme
                        "Fill Missing Values with -Unknown- in String Values", 
                        "Label Encoding", 
                        "One-Hot Encoding", 
                        "Fill Missing Values in Selected Columns", 
                        "Normalize Selected Columns", "Scaling Data"]
                                        
                operations = st.multiselect("Choose cleaning operations:", op)
                if "Delete Unnecessary Columns" in operations:
                        delete_columns = st.multiselect("Silinmesini istediÄŸiniz sÃ¼tunlarÄ± seÃ§in: ", df.columns.tolist())
                if "Find Columns with Missing Values" in operations:
                        missing_columns = st.multiselect("Eksik deÄŸerleri doldurmak istediÄŸiniz kolon isimleri", df.columns.tolist())
                if "Normalize Selected Columns" in operations:
                        normalize_columns = st.multiselect("Normalize etmek istediÄŸiniz kolon isimleri", df.columns.tolist())
                if "Scaling Data" in operations:
                        scaling_type = st.selectbox("Hangi scaling'i uygulamak istersiniz?", ["Min-Max Scaling", "Z-Score Scaling", "Robust Scaling", "MaxAbs Scaling"])
                
                
                if st.button("Clean"):
                        prepare_data(df, operations, delete_columns, missing_columns, normalize_columns, scaling_type)
                

#### FRONTEND
def frontend():
        st.header("Data Set Cleaning and Editing")
        st.write("Automates data cleaning steps such as filling in missing data and deleting unnecessary columns. It also allows the user to perform basic pre-processing steps such as Label Encoding or One-Hot Encoding. Model Selection and Training: By presenting multiple machine learning models to the user, it evaluates the performance of each with different parameters and gives the user the chance to choose the best performing model.")
        with st.expander("ğŸ‘€ Data Analysis"):
                st.write("Data analysis is the process of transforming raw data into meaningful information. It starts with collecting, cleaning, exploring and visualizing data. This is followed by data transformation and modeling. Analysis results are interpreted and reported and contribute to decision-making processes. Programming languages such as Python and R offer powerful tools for data analysis. Accurate data analysis helps businesses make strategic decisions and improve performance.")
        with st.expander("ğŸ”— Data Cleaning"):
                st.write("Data cleaning is the process of making raw data analyzable. It involves identifying and appropriately filling in missing data, correcting erroneous or inconsistent data, and identifying and addressing outliers. Data cleaning improves the accuracy and reliability of analysis results. Programming languages such as Python and R offer powerful tools and libraries for data cleaning. Proper data cleansing ensures the quality and reliability of the information businesses get from data analysis.")
        with st.expander("ğŸ” Dataset Search"):
                st.subheader("Streamlit Search")
                st.write("If you don't have a dataset or want a new one, you can use the dataset search.")
                # Excel dosyasÄ±nÄ± yÃ¼kleyin
                excel_file = 'media/datasets/advice_datasets.xlsx'
                df = pd.read_excel(excel_file)

                # Arama kutusu
                search_query = st.text_input("ğŸ” Search for a Dashboard", placeholder="Search...")

                if search_query:
                        # Arama terimini kÃ¼Ã§Ã¼k harflere Ã§evir
                        search_query = search_query.lower()
                        
                        # DataFrame iÃ§inde arama yap
                        results = df[df['DATASET NAME'].str.contains(search_query, case=False)]
                        
                        if not results.empty:
                                st.write(f"{len(results)} sonuÃ§ bulundu:")
                                for index, row in results.iterrows():
                                        st.write(f"**{row['DATASET NAME']}**: {row['LINK']}")
                        else:
                                st.write("AradÄ±ÄŸÄ±nÄ±z anahtar kelimeye uygun bir veri seti bulunamadÄ±. ")
                        
        st.info('If you want to know more in detail, read our [**manual**](https://www.retmon.com/blog/veri-gorsellestirme-nedir#:~:text=Veri%20g%C3%B6rselle%C5%9Ftirme%3B%20verileri%20insan%20beyninin,i%C3%A7in%20kullan%C4%B1lan%20teknikleri%20ifade%20eder.).', icon="â„¹ï¸")

        # Dosya yÃ¼kleme iÅŸlemi iÃ§in Streamlit'in file_uploader fonksiyonunu kullanma
        uploaded_file = st.file_uploader("Upload a file", type=["csv", "yaml", "json", "zip"])
        st.caption("You can upload your dataset in CSV, YAML, JSON and ZIP formats. The process is carried out by converting csv, yaml and json files to cv format. ZIP files are image files. Make sure that your ZIP file does not contain text or other contradictory files.")

        #Dosya tÃ¼rÃ¼nÃ¼ analiz eder ve dosya tÃ¼rÃ¼ json veya yaml ise csv dosyasÄ±na Ã§evirir.
        #Analiz ve temizleme iÅŸlemleri sekmeli sayfada yapÄ±lÄ±r.
        tab1, tab2, tab3, tab4 = st.tabs(["Dataset", "Information", "Graphics", "Cleaning"])                    
        with tab1:
                try:
                        if uploaded_file is None:
                                st.error('LÃ¼tfen bir veri seti yÃ¼kleyin. Veri setinizi eÄŸitim ve test iÃ§in hazÄ±rladÄ±ysanÄ±z, bir sonraki adÄ±ma geÃ§in.', icon="ğŸš¨")
                        else:
                                if uploaded_file.type == 'text/csv':  # CSV dosyasÄ± yÃ¼klenirse
                                        df = pd.read_csv(uploaded_file)
                                        if df.empty:
                                                st.error('YÃ¼klenen veri seti boÅŸ.', icon="ğŸš¨")
                                        else:
                                                row_of_datasets(df)

                                elif uploaded_file.type == 'text/yaml':  # YAML dosyasÄ± yÃ¼klenirse
                                        yaml_verisi = yaml.safe_load(uploaded_file)
                                        # YAML verisini DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼r
                                        df = pd.DataFrame(yaml_verisi)
                                        # CSV dosyasÄ±na kaydet
                                        df.to_csv("veri.csv", index=False)
                                        # CSV dosyasÄ±nÄ± tekrar yÃ¼kle
                                        df = pd.read_csv("veri.csv")
                                        row_of_datasets(df)

                                elif uploaded_file.type == 'application/json':  # JSON dosyasÄ± yÃ¼klenirse
                                        json_verisi = json.load(uploaded_file)
                                        # JSON verisini DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼r
                                        df = pd.DataFrame(json_verisi)
                                        # CSV dosyasÄ±na kaydet
                                        df.to_csv("veri.csv", index=False)
                                        # CSV dosyasÄ±nÄ± tekrar yÃ¼kle
                                        df = pd.read_csv("veri.csv")
                                        row_of_datasets(df)

                                elif uploaded_file.type in ['application/zip', 'application/x-zip-compressed']:  # Zip dosyasÄ± yÃ¼klenirse (resim klasÃ¶rÃ¼)
                                        with zipfile.ZipFile(uploaded_file, 'r') as zip_ref:
                                                zip_ref.extractall("extracted_images")
                                        st.success('Resim klasÃ¶rÃ¼ baÅŸarÄ±yla yÃ¼klendi ve aÃ§Ä±ldÄ±.')

                                        folder_data = analyze_image_folders("extracted_images")
                                        #display_analysis(folder_data)
                                        display_sample_images("extracted_images", folder_data)
                                else:
                                        st.write("Dosya formatÄ± desteklenmiyor.")


                except (pd.errors.EmptyDataError, pd.errors.ParserError):

                        st.error('YÃ¼klenen veri seti boÅŸ veya geÃ§ersiz.', icon="ğŸš¨")
                except Exception as e:
                        st.error(f'Bir hata oluÅŸtu: {e}', icon="ğŸš¨")
        with tab2:
                try:
                        if uploaded_file is None:
                                st.error('Please upload a dataset. If your dataset prepare for train and testing, go to next  step.', icon="ğŸš¨")
                        
                        else:
                                if uploaded_file.type in ['application/zip', 'application/x-zip-compressed']:  # Zip dosyasÄ± ise
                                        folder_data = analyze_image_folders("extracted_images")
                                        display_analysis(folder_data)
                                elif df.empty:
                                        st.error('The uploaded dataset is empty.', icon="ğŸš¨")
                                else:
                                        information(df)
                except (pd.errors.EmptyDataError, pd.errors.ParserError):
                        st.error('YÃ¼klenen veri seti boÅŸ veya geÃ§ersiz.', icon="ğŸš¨")
                except Exception as e:
                        st.error(f'Bir hata oluÅŸtu: {e}', icon="ğŸš¨")
        with tab3:
                try:
                        if uploaded_file is None:
                                st.error('Please upload a dataset. If your dataset prepare for train and testing, go to next  step.', icon="ğŸš¨")
                        else:
                                if uploaded_file.type in ['application/zip', 'application/x-zip-compressed']:  # Zip dosyasÄ± ise
                                        folder_data = analyze_image_folders("extracted_images")
                                        plot_image_statistics(folder_data)
                                else:
                                        gr = st.selectbox("Hangi grafiÄŸi gÃ¶rÃ¼ntÃ¼lemek istiyorsun?", ["Line Chart", "Bar Chart", "Area Chart", "Scatter Chart"])
                                        column_c = st.multiselect("GrafiÄŸe dÃ¶nÃ¼ÅŸtÃ¼rmek istediÄŸiniz sÃ¼tunlarÄ± seÃ§in.", df.columns.tolist() )
                                        if gr == "Line Chart":
                                                st.caption("Line chart, verilerin bir Ã§izgi grafiÄŸi Ã¼zerinde gÃ¶sterildiÄŸi bir grafik tÃ¼rÃ¼dÃ¼r. X ekseni genellikle zaman veya kategorik deÄŸerlerle iliÅŸkilendirilirken, Y ekseni ise bu deÄŸerlerin karÅŸÄ±lÄ±k geldiÄŸi Ã¶lÃ§Ã¼lebilir verileri temsil eder. Her veri noktasÄ±, X ekseni boyunca konumlandÄ±rÄ±lÄ±r ve belirli bir Y ekseni deÄŸerine sahiptir. Bu noktalar, birbirlerine doÄŸrusal olarak baÄŸlanarak bir Ã§izgi oluÅŸturulur. Line chart, veri setindeki deÄŸiÅŸiklikleri izlemek, trendleri analiz etmek ve iliÅŸkileri anlamak iÃ§in kullanÄ±lÄ±r. Ã–zellikle zaman serileri verileriyle Ã§alÄ±ÅŸÄ±rken, belirli bir zaman aralÄ±ÄŸÄ±ndaki deÄŸiÅŸimleri anlamak iÃ§in line chart Ã§ok kullanÄ±ÅŸlÄ±dÄ±r.")
                                                chart_data = pd.DataFrame(np.random.randn(20, len(column_c)), columns=column_c)
                                                st.line_chart(chart_data)
                                        elif gr == "Bar Chart":
                                                st.caption("Line chart, verilerin bir Ã§izgi grafiÄŸi Ã¼zerinde gÃ¶sterildiÄŸi bir grafik tÃ¼rÃ¼dÃ¼r. X ekseni genellikle zaman veya kategorik deÄŸerlerle iliÅŸkilendirilirken, Y ekseni ise bu deÄŸerlerin karÅŸÄ±lÄ±k geldiÄŸi Ã¶lÃ§Ã¼lebilir verileri temsil eder. Her veri noktasÄ±, X ekseni boyunca konumlandÄ±rÄ±lÄ±r ve belirli bir Y ekseni deÄŸerine sahiptir. Bu noktalar, birbirlerine doÄŸrusal olarak baÄŸlanarak bir Ã§izgi oluÅŸturulur. Line chart, veri setindeki deÄŸiÅŸiklikleri izlemek, trendleri analiz etmek ve iliÅŸkileri anlamak iÃ§in kullanÄ±lÄ±r. Ã–zellikle zaman serileri verileriyle Ã§alÄ±ÅŸÄ±rken, belirli bir zaman aralÄ±ÄŸÄ±ndaki deÄŸiÅŸimleri anlamak iÃ§in line chart Ã§ok kullanÄ±ÅŸlÄ±dÄ±r.")
                                                chart_data = pd.DataFrame(np.random.randn(30, len(column_c)), columns=column_c)
                                                st.bar_chart(chart_data)
                                        elif gr == "Area Chart":
                                                st.caption("Line chart, verilerin bir Ã§izgi grafiÄŸi Ã¼zerinde gÃ¶sterildiÄŸi bir grafik tÃ¼rÃ¼dÃ¼r. X ekseni genellikle zaman veya kategorik deÄŸerlerle iliÅŸkilendirilirken, Y ekseni ise bu deÄŸerlerin karÅŸÄ±lÄ±k geldiÄŸi Ã¶lÃ§Ã¼lebilir verileri temsil eder. Her veri noktasÄ±, X ekseni boyunca konumlandÄ±rÄ±lÄ±r ve belirli bir Y ekseni deÄŸerine sahiptir. Bu noktalar, birbirlerine doÄŸrusal olarak baÄŸlanarak bir Ã§izgi oluÅŸturulur. Line chart, veri setindeki deÄŸiÅŸiklikleri izlemek, trendleri analiz etmek ve iliÅŸkileri anlamak iÃ§in kullanÄ±lÄ±r. Ã–zellikle zaman serileri verileriyle Ã§alÄ±ÅŸÄ±rken, belirli bir zaman aralÄ±ÄŸÄ±ndaki deÄŸiÅŸimleri anlamak iÃ§in line chart Ã§ok kullanÄ±ÅŸlÄ±dÄ±r.")
                                                chart_data = pd.DataFrame(np.random.randn(20, len(column_c)), columns=column_c)
                                                st.area_chart(chart_data)
                                        elif gr == "Scatter Chart":
                                                st.caption("Line chart, verilerin bir Ã§izgi grafiÄŸi Ã¼zerinde gÃ¶sterildiÄŸi bir grafik tÃ¼rÃ¼dÃ¼r. X ekseni genellikle zaman veya kategorik deÄŸerlerle iliÅŸkilendirilirken, Y ekseni ise bu deÄŸerlerin karÅŸÄ±lÄ±k geldiÄŸi Ã¶lÃ§Ã¼lebilir verileri temsil eder. Her veri noktasÄ±, X ekseni boyunca konumlandÄ±rÄ±lÄ±r ve belirli bir Y ekseni deÄŸerine sahiptir. Bu noktalar, birbirlerine doÄŸrusal olarak baÄŸlanarak bir Ã§izgi oluÅŸturulur. Line chart, veri setindeki deÄŸiÅŸiklikleri izlemek, trendleri analiz etmek ve iliÅŸkileri anlamak iÃ§in kullanÄ±lÄ±r. Ã–zellikle zaman serileri verileriyle Ã§alÄ±ÅŸÄ±rken, belirli bir zaman aralÄ±ÄŸÄ±ndaki deÄŸiÅŸimleri anlamak iÃ§in line chart Ã§ok kullanÄ±ÅŸlÄ±dÄ±r.")
                                                chart_data = pd.DataFrame(np.random.randn(30, len(column_c)), columns=column_c)
                                                st.scatter_chart(chart_data)
                                        else:
                                                pass
                except (pd.errors.EmptyDataError, pd.errors.ParserError):
                        st.error('YÃ¼klenen veri seti boÅŸ veya geÃ§ersiz.', icon="ğŸš¨")
                except Exception as e:
                        st.error(f'Bir hata oluÅŸtu: {e}', icon="ğŸš¨")              
        with tab4:
                try:
                        if uploaded_file is None:
                                st.error('Please upload a dataset. If your dataset prepare for train and testing, go to next  step.', icon="ğŸš¨")
                        else:
                                cleaning = st.selectbox("Do you want to clean the data yourself or have it cleaned automatically?", ["Choose", "Auto Dataset Cleaning", "Cleaning the Dataset Yourself"])

                
                                if cleaning == "Auto Dataset Cleaning":
                                        auto_cleaning(df)
                                elif cleaning == "Cleaning the Dataset Yourself":
                                        myself_cleaning(df)
                                else:
                                        st.write("Please select a valid data clearing option.")

                                
                except (pd.errors.EmptyDataError, pd.errors.ParserError):
                        st.error('YÃ¼klenen veri seti boÅŸ veya geÃ§ersiz.', icon="ğŸš¨")
                except Exception as e:
                        st.error(f'Bir hata oluÅŸtu: {e}', icon="ğŸš¨")
                                
if __name__ == "__main__":
       # Sayfa adÄ±nÄ± ve sayfanÄ±n geniÅŸ olmasÄ±nÄ± saÄŸlama
        st.set_page_config(page_title="ATOM AI", layout="wide", initial_sidebar_state="expanded")
        
        frontend()

        