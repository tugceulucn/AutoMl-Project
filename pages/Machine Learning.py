import zipfile
import json, time
import inspect
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from itertools import product

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import streamlit as st
import yaml, joblib, pickle

from bokeh.plotting import figure
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, \
    RandomForestClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.impute import SimpleImputer
from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, mean_squared_error, precision_score, recall_score, roc_auc_score
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier

from streamlit_card import card
from streamlit_lottie import st_lottie
from streamlit_option_menu import option_menu
from warnings import filterwarnings
from sklearn.exceptions import ConvergenceWarning
import time

filterwarnings('ignore', category=ConvergenceWarning)

def handle_file_upload(uploaded_file):
    try:
        if uploaded_file is None:
            st.error('LÃ¼tfen bir veri seti yÃ¼kleyin. Veri setinizi eÄŸitim ve test iÃ§in hazÄ±rladÄ±ysanÄ±z, bir sonraki adÄ±ma geÃ§in.', icon="ğŸš¨")
        else:
            if uploaded_file.type == 'text/csv':  # CSV dosyasÄ± yÃ¼klenirse
                df = pd.read_csv(uploaded_file)
                st.success("CSV Data loaded successfully. Let's continue with Machine Learning!", icon="âœ…")
            elif uploaded_file.type == 'text/yaml':  # YAML dosyasÄ± yÃ¼klenirse
                    yaml_verisi = yaml.safe_load(uploaded_file)
                    # YAML verisini DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼r
                    df = pd.DataFrame(yaml_verisi)
                    # CSV dosyasÄ±na kaydet
                    df.to_csv("veri.csv", index=False)
                    # CSV dosyasÄ±nÄ± tekrar yÃ¼kle
                    df = pd.read_csv("veri.csv")
                    st.success("YAML Data loaded successfully.Let's continue with Machine Learning!", icon="âœ…")
            elif uploaded_file.type == 'application/json':  # JSON dosyasÄ± yÃ¼klenirse
                    json_verisi = json.load(uploaded_file)
                    # JSON verisini DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼r
                    df = pd.DataFrame(json_verisi)
                    # CSV dosyasÄ±na kaydet
                    df.to_csv("veri.csv", index=False)
                    # CSV dosyasÄ±nÄ± tekrar yÃ¼kle
                    df = pd.read_csv("veri.csv")
                    st.success("JSON Data loaded successfully.Let's continue with Machine Learning!", icon="âœ…")
            elif uploaded_file.type in ['application/zip', 'application/x-zip-compressed']:  # Zip dosyasÄ± yÃ¼klenirse (resim klasÃ¶rÃ¼)
                    with zipfile.ZipFile(uploaded_file, 'r') as zip_ref:
                        zip_ref.extractall("extracted_images")
                    st.success("ZIP/Image folder has been successfully uploaded and opened.Let's continue with Machine Learning!", icon="âœ…")
            else:
                st.write("The file format is not supported.")
                
            if df.empty:
                    st.error('The loaded data set is empty.', icon="ğŸš¨")
            
            return df
        
    except (pd.errors.EmptyDataError, pd.errors.ParserError):
        st.error("No columns to parse from file. Please check the file content.", icon="ğŸš¨")
    except Exception as e:
        st.error(f"Error reading the file: {e}", icon="ğŸš¨")   

def find_best_params_classification(name, model, X, Y, test_size_range, random_state_range, max_depth_range):
    best_score = float('-inf')
    best_params = {}

    for test_size, random_state, max_depth in product(test_size_range, random_state_range, max_depth_range):
        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state)

        model_instance = model

        if hasattr(model_instance, 'random_state'):
            model_instance.set_params(random_state=random_state)
        if hasattr(model_instance, 'max_depth'):
            model_instance.set_params(max_depth=max_depth)

        model_instance.fit(X_train, Y_train)
        Y_pred = model_instance.predict(X_test)
        score = accuracy_score(Y_test, Y_pred)

        if score > best_score:
            best_score = score
            best_params = {'test_size': test_size, 'random_state': random_state, 'max_depth': max_depth}

    return {'name': name, 'best_params': best_params, 'best_score': best_score}

def find_best_params_regression(name, model, X, Y, test_size_range, random_state_range):
    best_score = float('inf')
    best_params = {}

    for test_size, random_state in product(test_size_range, random_state_range):
        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state)

        model_instance = model

        if hasattr(model_instance, 'random_state'):
            model_instance.set_params(random_state=random_state)

        model_instance.fit(X_train, Y_train)
        Y_pred = model_instance.predict(X_test)
        score = mean_squared_error(Y_test, Y_pred)

        if score < best_score:
            best_score = score
            best_params = {'test_size': test_size, 'random_state': random_state}

    return {'name': name, 'best_params': best_params, 'best_score': best_score}

def save_model(model, format_choice, filename):
    if format_choice == 'joblib':
        joblib.dump(model, filename)
        st.write(f"Model baÅŸarÄ±yla '{filename}' adlÄ± dosyaya kaydedildi.")
    elif format_choice == 'pickle':
        with open(filename, 'wb') as f:
            pickle.dump(model, f)
        st.write(f"Model baÅŸarÄ±yla '{filename}' adlÄ± dosyaya kaydedildi.")
    elif format_choice == 'onnx':
        st.write("Bu model ONNX formatÄ±nda kaydedilemez.")
    else:
        st.write("GeÃ§ersiz format seÃ§imi! LÃ¼tfen 'joblib', 'pickle', 'h5', 'onnx', 'json' veya 'yaml' ÅŸeklinde bir format seÃ§in.")

def data_preprocessing(df):
    try:
        # Yinelenen satÄ±rlarÄ± kaldÄ±rma
        df.drop_duplicates(inplace=True)
        # Label Encoding iÅŸlemi burada gerÃ§ekleÅŸecek
        label_encoder = LabelEncoder()
        string_columns = df.select_dtypes(include=['object']).columns
        df[string_columns] = df[string_columns].apply(label_encoder.fit_transform)
        # Integer deÄŸerlere ortalama ile eksik deÄŸerleri doldurma
        integer_columns = df.select_dtypes(include=['int', 'float']).columns
        df[integer_columns] = df[integer_columns].fillna(df[integer_columns].mean())
        # String deÄŸerlere "Bilinmiyor" ile eksik deÄŸerleri doldurma
        string_columns = df.select_dtypes(include=['object']).columns
        df[string_columns] = df[string_columns].fillna("Bilinmiyor")
        
        return df, label_encoder
    
    except Exception as e:
        st.write(f"Hata: {e}")
        return None

def predict_new_data(model, columns, label_encoder):
    new_data = {}
    i = 0
    
    while i < len(columns):
        column = columns[i]
        value = st.text_input(f"{column}: ", key=f"input_{i}")
        if st.button("SÄ±radaki", key=f"button_{i}"):
            if value == "":
                st.error(f"LÃ¼tfen {column} iÃ§in bir deÄŸer girin.")
            else:
                new_data[column] = [value if not value.replace('.', '', 1).isdigit() else float(value)]
                i += 1

    if len(new_data) == len(columns):
        new_df = pd.DataFrame(new_data)
        
        # Yeni verilerin dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesi
        string_columns = new_df.select_dtypes(include=['object']).columns
        new_df[string_columns] = new_df[string_columns].apply(label_encoder.fit_transform)
        new_df = new_df.reindex(columns=columns, fill_value=0)

        if not new_df.empty:
            prediction = model.predict(new_df)
            st.write("Tahmin edilen hedef deÄŸiÅŸken:", prediction)
        else:
            st.error("Yeterli veri giriÅŸi yapÄ±lmadÄ±.")
    else:
        st.info("TÃ¼m alanlarÄ± doldurduÄŸunuzdan emin olun.")

#MAKÄ°NE Ã–ÄRENMESÄ°
def automl(df):
    try:
        if df is None:
            st.error('Please upload a dataset. If your dataset is prepared for training and testing, go to the next step.', icon="ğŸš¨")
            return      
        with st.container(border=True):
            st.info("Ä°ÅŸleminiz bir sÃ¼re devam edecek. Haydi baÅŸlayalÄ±m!", icon='ğŸ‰')
            df, label = data_preprocessing(df)

            # Model seÃ§enekleri ve kÄ±saltmalarÄ±
            classification_models = [
                ("LR", LogisticRegression()),
                ("LDA", LinearDiscriminantAnalysis()),
                ("KNN", KNeighborsClassifier()),
                ("DT", DecisionTreeClassifier()),
                ("NB", GaussianNB()),
                ("SVM", SVC()),
                ("RF", RandomForestClassifier()),
                ("GB", GradientBoostingClassifier()),
                ("XGB", XGBClassifier()),
                ("LGBM", LGBMClassifier()),
                ("CatBoost", CatBoostClassifier()),
                ("MLP", MLPClassifier()),
                ("AdaBoost", AdaBoostClassifier()),
                ("Bagging", BaggingClassifier()),
                ("ExtraTrees", ExtraTreesClassifier()),
                ("GaussianProcess", GaussianProcessClassifier())
            ]
            regression_models = [
                ("LinearRegression", LinearRegression()),
                ("Ridge", Ridge()),
                ("Lasso", Lasso()),
                ("ElasticNet", ElasticNet())
            ]

            # Hedef deÄŸiÅŸkeni seÃ§imi ve problem tÃ¼rÃ¼ seÃ§imi
            problem_type = st.selectbox("Problemin tÃ¼rÃ¼nÃ¼ seÃ§in:", options=['SÄ±nÄ±flandÄ±rma', 'Regresyon'])
            hedef_degisken = st.selectbox("Hedef DeÄŸiÅŸkeni SeÃ§in", df.columns.tolist())
            #st.write(problem_type)

            if hedef_degisken in df.columns:
                X = df.drop(hedef_degisken, axis=1)  # BaÄŸÄ±msÄ±z deÄŸiÅŸkenler
                Y = df[hedef_degisken]  # Hedef deÄŸiÅŸken
            else:
                st.write("Hedef deÄŸiÅŸken adÄ± geÃ§ersiz. LÃ¼tfen mevcut bir hedef deÄŸiÅŸken adÄ± girin.")
                return
            
            if problem_type == "SÄ±nÄ±flandÄ±rma":
                all_models = [name for name, _ in classification_models]
                first_model = st.selectbox("Ä°lk modeli seÃ§in:", all_models, index=0)
                second_model = st.selectbox("Ä°kinci modeli seÃ§in:", all_models, index=0)
                selected_models = [first_model, second_model]
                
            else:
                selected_models = ['LinearRegression', 'Ridge', 'Lasso', 'ElasticNet']

            ml_start = st.button("Makine Ã–ÄŸrenmesini BaÅŸlat.")
            
            if ml_start:
                st.info("Makine Ã¶ÄŸrenmesi baÅŸlatÄ±ldÄ±. LÃ¼tfen sonuÃ§larÄ± bekleyiniz.", icon='ğŸ¤–')

                if problem_type == 'SÄ±nÄ±flandÄ±rma':
                    st.info("SÄ±nÄ±flandÄ±rma yapÄ±lÄ±yor.", icon="â„¹ï¸")
                    models = [model for name, model in classification_models if name in selected_models]
                    find_best_params = find_best_params_classification
                    st.info("SÄ±nÄ±flandÄ±rma best parametreleri bulundu.", icon="â„¹ï¸")
                    
                    test_size_range = [0.1, 0.2, 0.3, 0.4]
                    random_state_range = [42, 2021, 12345]
                    max_depth_range = [3, 5, 7, 9]
                
                elif problem_type == 'Regresyon':
                    st.info("Regresyon yapÄ±lÄ±yor.", icon="â„¹ï¸")
                    models = [model for name, model in regression_models if name in selected_models]
                    find_best_params = find_best_params_regression
                    st.info("Regresyon best parametreleri bulundu. Bekleyiniz.", icon="â„¹ï¸")
                    
                    test_size_range = [0.1, 0.2, 0.3, 0.4]
                    random_state_range = [42, 2021, 12345]
                    max_depth_range = []
                
                futures = []
                if problem_type == 'SÄ±nÄ±flandÄ±rma':
                    with ProcessPoolExecutor() as executor:
                        for name, alg in classification_models:
                            if name in selected_models:
                                futures.append(
                                    executor.submit(find_best_params, name, alg, X, Y, test_size_range, random_state_range, max_depth_range))
                elif problem_type == 'Regresyon':
                    with ProcessPoolExecutor() as executor:
                        for name, alg in regression_models:
                            if name in selected_models:
                                futures.append(
                                    executor.submit(find_best_params, name, alg, X, Y, test_size_range, random_state_range))
                
                results = []
                for future in as_completed(futures):
                    result = future.result()
                    results.append(result)

                df_models = pd.DataFrame({
                    "Models": selected_models,
                    })
                
                # SonuÃ§larÄ± DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼rme
                df_results = pd.DataFrame(results)

                #    Streamlit arayÃ¼zÃ¼nde tabloyu gÃ¶sterme
                st.write("SeÃ§ilen Modeller")
                st.write(df_models)
                st.write("SonuÃ§lar")
                st.write(df_results)
       
    except Exception as e:
        st.write(f"Hata: {e}")

def manualml(df):
    if df is None:
        st.error('Please upload a dataset. If your dataset is prepared for training and testing, go to the next step.', icon="ğŸš¨")
        return
    
    st.info("Ä°ÅŸleminiz bir sÃ¼re devam edecek. Haydi baÅŸlayalÄ±m!", icon='ğŸ‰')
    df, Label_Encoder= data_preprocessing(df)  # data_preprocessing fonksiyonu tanÄ±mlanmalÄ± veya bu satÄ±r kaldÄ±rÄ±lmalÄ±

    # KullanÄ±labilir modeller ve bunlarÄ±n adlarÄ±
    models = [
        ("LR", LogisticRegression),
        ("LIR", LinearRegression),
        ("LDA", LinearDiscriminantAnalysis),
        ("KNN", KNeighborsClassifier),
        ("DT", DecisionTreeClassifier),
        ("NB", GaussianNB),
        ("SVM", SVC),
        ("RF", RandomForestClassifier),
        ("GB", GradientBoostingClassifier),
        ("XGB", XGBClassifier),
        ("LGBM", LGBMClassifier),
        ("CatBoost", CatBoostClassifier),
        ("MLP", MLPClassifier),
        ("AdaBoost", AdaBoostClassifier),
        ("Bagging", BaggingClassifier),
        ("ExtraTrees", ExtraTreesClassifier),
        ("GaussianProcess", GaussianProcessClassifier),
        ("Ridge Regression", Ridge),
        ("Lasso Regression", Lasso),
        ("ElasticNet Regresyon", ElasticNet)
    ]

    # X ve y ayrÄ±ÅŸtÄ±rma
    target_variable = st.selectbox("Hedef deÄŸiÅŸkeni seÃ§in:", df.columns)
    X = df.drop(columns=[target_variable])
    y = df[target_variable]

    # KullanÄ±cÄ±dan test_size ve random_state deÄŸerlerini al
    col1, col2, col3 = st.columns(3)
    with col1:
        test_size = st.number_input("Test setinin oranÄ±nÄ± girin (Ã¶rn: 0.2):", min_value=0.1, max_value=0.9, step=0.1, value=0.2)
    with col2:
        random_state = st.number_input("Random state deÄŸerini girin (Ã¶rn: 42):", min_value=30, step=2, value=30)
    with col3:
        # KullanÄ±cÄ±dan model seÃ§mesini iste
        all_models = [name for name, _ in models]
        selected_model_name = st.selectbox("Modelleri SeÃ§in (Maksimum 2)", all_models)
        selected_model_class = [model for name, model in models if name in selected_model_name]

    scol1, scol2 = st.columns(2)
    with scol1:
        max_depth = st.number_input("max_depth oranÄ±nÄ± girin (Ã¶rn: 0.2):", min_value=1, max_value=50, step=2, value=1)
    with scol2:
        n_estimators = st.number_input("n_estimators oranÄ±nÄ± girin (Ã¶rn: 0.2):", min_value=10, max_value=100, step=5, value=10)

    if len(selected_model_class) == 0:
        st.error("LÃ¼tfen en az bir model seÃ§in.", icon="ğŸš¨")
        return

    trainModel = st.button("Modeli EÄŸit.")

    if trainModel:
        selected_model_class = selected_model_class[0]

        # SeÃ§ilen modelin parametrelerini kullanÄ±cÄ±ya gÃ¶ster
        params = {}
        st.write(f"SeÃ§ilen model: {selected_model_name}")
        st.write("Bu modelin alabileceÄŸi parametreler:")

        # Model parametrelerini kullanÄ±cÄ±dan al
        if 'n_estimators' in selected_model_class().get_params():
            params['n_estimators'] = n_estimators
        if 'random_state' in selected_model_class().get_params():
            params['random_state'] = random_state
        if 'max_depth' in selected_model_class().get_params():
            params['max_depth'] = max_depth

        st.write(params)

        # Modeli parametrelerle oluÅŸtur
        model = selected_model_class(**params)
        st.write(model)

        # Veriyi eÄŸitim ve test setine ayÄ±r
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)

        # Modeli eÄŸit ve test et
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        # Modelin parametrelerini yazdÄ±rma
        # st.write("Model Parametreleri:", model.get_params())

        # SonuÃ§larÄ± deÄŸerlendir
        if selected_model_name in ["LIR", "Ridge Regression", "Lasso Regression", "ElasticNet Regresyon"]:
            # Regresyon modelleri iÃ§in
            mse = mean_squared_error(y_test, y_pred)
            st.write("Mean Squared Error:", mse)
        else:
            # SÄ±nÄ±flandÄ±rma modelleri iÃ§in
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted')
            recall = recall_score(y_test, y_pred, average='weighted')
            f1 = f1_score(y_test, y_pred, average='weighted')

            # SonuÃ§larÄ± yazdÄ±rma
            st.write("Accuracy:", accuracy)
            st.write("Precision:", precision)
            st.write("Recall:", recall)
            st.write("F1 Score:", f1)

        # Yeni veri ile tahmin yapma
        predict_new_data(model, X.columns, Label_Encoder)

        # Modeli kaydetme
        save_choice = st.selectbox("Modeli kaydetmek ister misiniz?", ["Evet", "HayÄ±r"])
        if save_choice == "Evet":
            format_choice = st.selectbox("LÃ¼tfen kaydetmek istediÄŸiniz dosya formatÄ±nÄ± seÃ§in:", ["joblib", "pickle", "onnx"])
            if format_choice == "joblib":
                joblib.dump(model, "model_ATOMai.pkl")
                with open("model_ATOMai.pkl", 'rb') as f:
                    st.download_button("Modeli Ä°ndir", f, file_name="model_ATOMai.pkl")
            elif format_choice == "pickle":
                with open("model_ATOMai.pkl", 'wb') as f:
                    pickle.dump(model, f)
                with open("model_ATOMai.pkl", 'rb') as f:
                    st.download_button("Modeli Ä°ndir", f, file_name="model_ATOMai.pkl")
            elif format_choice == "onnx":
                # ONNX modeli kaydetme kodu burada eklenebilir
                text_contents = "Bu, bir text dosyasÄ±dÄ±r"
                st.download_button("Download some text", text_contents)
        else:
            st.write("Makine Ã–ÄŸrenmesi sonlanmÄ±ÅŸtÄ±r.")
            
        #filename = input("Kaydetmek istediÄŸiniz dosyanÄ±n adÄ±nÄ± girin (Ã¶rn: model.pkl, model.h5, model.onnx, model.json, model.yaml): ")
        #save_model(model, format_choice, filename)   

# FRONTEND
def frontend():
    st.subheader("Machine Learning")
    st.write("Makine Ã¶ÄŸrenmesi, bilgisayarlarÄ±n aÃ§Ä±kÃ§a programlanmadan verilerden Ã¶ÄŸrenmesini ve tahminler yapmasÄ±nÄ± saÄŸlayan bir yapay zeka alt alanÄ±dÄ±r. Bu sÃ¼reÃ§te, bilgisayarlar verilere dayalÄ± olarak kalÄ±plarÄ± ve iliÅŸkileri Ã¶ÄŸrenir ve bu Ã¶ÄŸrenme sonucunda gelecekteki verilere uygulanan tahminlerde bulunurlar. Makine Ã¶ÄŸrenmesi, Ã§eÅŸitli algoritmalar ve teknikler kullanarak verilerden anlamlÄ± sonuÃ§lar Ã§Ä±karÄ±r. Makine Ã¶ÄŸrenmesinin temel bileÅŸenleri veri, model ve algoritmadÄ±r. Veri, modelin Ã¶ÄŸrenmesi iÃ§in kullanÄ±lan Ã¶rnekleri iÃ§erir. Model, verilerden Ã¶ÄŸrenilen matematiksel bir temsildir. Algoritma ise modeli eÄŸitmek iÃ§in kullanÄ±lan yÃ¶ntemdir. Makine Ã¶ÄŸrenmesi, Ã§ok Ã§eÅŸitli alanlarda kullanÄ±lÄ±r. Tahmin ve sÄ±nÄ±flandÄ±rma en yaygÄ±n kullanÄ±m alanlarÄ± arasÄ±ndadÄ±r. Makine Ã¶ÄŸrenmesi, bÃ¼yÃ¼k miktarda veriyi analiz etmek ve bu verilere dayalÄ± kararlar almak iÃ§in gÃ¼Ã§lÃ¼ bir araÃ§tÄ±r. Bu, iÅŸ dÃ¼nyasÄ±ndan saÄŸlÄ±k sektÃ¶rÃ¼ne kadar birÃ§ok alanda bÃ¼yÃ¼k avantajlar saÄŸlar. Makine Ã¶ÄŸrenmesi sayesinde, daha doÄŸru tahminler yapÄ±labilir, verimlilik artÄ±rÄ±labilir ve yeni keÅŸifler yapÄ±labilir.")
    with st.expander("ğŸ”— Classification"):
        st.write("Veri iÅŸleme ve model seÃ§imi iÅŸlemlerini otomatikleÅŸtirmek iÃ§in bu aracÄ± kullanabilirsiniz.")
    with st.expander("ğŸ”— Regression"):
        st.write("Veri iÅŸleme ve model seÃ§imi iÅŸlemlerini otomatikleÅŸtirmek iÃ§in bu aracÄ± kullanabilirsiniz.")
    
    
    st.info('If you want to know more in detail, read our [**manual**](https://www.retmon.com/blog/veri-gorsellestirme-nedir#:~:text=Veri%20g%C3%B6rselle%C5%9Ftirme%3B%20verileri%20insan%20beyninin,i%C3%A7in%20kullan%C4%B1lan%20teknikleri%20ifade%20eder.).', icon="â„¹ï¸")
   
    # Dosya yÃ¼kleme iÅŸlemi iÃ§in Streamlit'in file_uploader fonksiyonunu kullanma
    uploaded_file = st.file_uploader("Upload a file", type=["csv", "yaml", "json", "zip"])

    ml_option = st.selectbox("Do you want to do your training yourself or should it be done automatically?", ["Choose", "I'll do it myself.", "Do it."])

    if uploaded_file is None:
        st.error('Please upload a dataset. If your dataset is prepared for training and testing, go to the next step.', icon="ğŸš¨")
    elif ml_option == "I'll do it myself.":
        df = handle_file_upload(uploaded_file)
        manualml(df)
    elif ml_option == "Do it.":
        df = handle_file_upload(uploaded_file)
        automl(df)
    else:
        st.write("Please made an valid selection.")

if __name__ == "__main__":
    # Page configuration
    st.set_page_config(page_title="ATOM AI", layout="wide",)
    # YakÄ±nsama uyarÄ±larÄ±nÄ± bastÄ±rmak iÃ§in ayarlarÄ± yapÄ±landÄ±rma
    filterwarnings("ignore", category=ConvergenceWarning)

    #Frontend ile arayÃ¼zÃ¼n oluÅŸturulmasÄ±.
    frontend()
