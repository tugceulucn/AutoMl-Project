import zipfile
import json, time
import inspect
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from itertools import product

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import streamlit as st
import yaml, joblib, pickle

from bokeh.plotting import figure
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, \
    RandomForestClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.impute import SimpleImputer
from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, mean_squared_error, precision_score, recall_score, roc_auc_score
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier

from streamlit_card import card
from streamlit_lottie import st_lottie
from streamlit_option_menu import option_menu
from warnings import filterwarnings
from sklearn.exceptions import ConvergenceWarning
import time

filterwarnings('ignore', category=ConvergenceWarning)

#GENEL FONKSÄ°YONLAR
def handle_file_upload(uploaded_file):
    try:
        if uploaded_file is None:
            st.error('LÃ¼tfen bir veri seti yÃ¼kleyin. Veri setinizi eÄŸitim ve test iÃ§in hazÄ±rladÄ±ysanÄ±z, bir sonraki adÄ±ma geÃ§in.', icon="ğŸš¨")
        else:
            if uploaded_file.type == 'text/csv':  # CSV dosyasÄ± yÃ¼klenirse
                df = pd.read_csv(uploaded_file)
                st.success("CSV Data loaded successfully. Let's continue with Machine Learning!", icon="âœ…")
            elif uploaded_file.type == 'text/yaml':  # YAML dosyasÄ± yÃ¼klenirse
                    yaml_verisi = yaml.safe_load(uploaded_file)
                    # YAML verisini DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼r
                    df = pd.DataFrame(yaml_verisi)
                    # CSV dosyasÄ±na kaydet
                    df.to_csv("veri.csv", index=False)
                    # CSV dosyasÄ±nÄ± tekrar yÃ¼kle
                    df = pd.read_csv("veri.csv")
                    st.success("YAML Data loaded successfully.Let's continue with Machine Learning!", icon="âœ…")
            elif uploaded_file.type == 'application/json':  # JSON dosyasÄ± yÃ¼klenirse
                    json_verisi = json.load(uploaded_file)
                    # JSON verisini DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼r
                    df = pd.DataFrame(json_verisi)
                    # CSV dosyasÄ±na kaydet
                    df.to_csv("veri.csv", index=False)
                    # CSV dosyasÄ±nÄ± tekrar yÃ¼kle
                    df = pd.read_csv("veri.csv")
                    st.success("JSON Data loaded successfully.Let's continue with Machine Learning!", icon="âœ…")
            elif uploaded_file.type in ['application/zip', 'application/x-zip-compressed']:  # Zip dosyasÄ± yÃ¼klenirse (resim klasÃ¶rÃ¼)
                    with zipfile.ZipFile(uploaded_file, 'r') as zip_ref:
                        zip_ref.extractall("extracted_images")
                    st.success("ZIP/Image folder has been successfully uploaded and opened.Let's continue with Machine Learning!", icon="âœ…")
            else:
                st.write("The file format is not supported.")
                
            if df.empty:
                    st.error('The loaded data set is empty.', icon="ğŸš¨")
            
            return df
        
    except (pd.errors.EmptyDataError, pd.errors.ParserError):
        st.error("No columns to parse from file. Please check the file content.", icon="ğŸš¨")
    except Exception as e:
        st.error(f"Error reading the file: {e}", icon="ğŸš¨")   

def data_preprocessing(df):
    try:
        # Yinelenen satÄ±rlarÄ± kaldÄ±rma
        df.drop_duplicates(inplace=True)
        # Label Encoding iÅŸlemi burada gerÃ§ekleÅŸecek
        label_encoder = LabelEncoder()
        string_columns = df.select_dtypes(include=['object']).columns
        df[string_columns] = df[string_columns].apply(label_encoder.fit_transform)
        # Integer deÄŸerlere ortalama ile eksik deÄŸerleri doldurma
        integer_columns = df.select_dtypes(include=['int', 'float']).columns
        df[integer_columns] = df[integer_columns].fillna(df[integer_columns].mean())
        # String deÄŸerlere "Bilinmiyor" ile eksik deÄŸerleri doldurma
        string_columns = df.select_dtypes(include=['object']).columns
        df[string_columns] = df[string_columns].fillna("Bilinmiyor")
        
        return df, label_encoder
    
    except Exception as e:
        st.write(f"Hata: {e}")
        return None

#AUTOML FONKSÄ°YONLARI
def find_best_params_classification(name, model, X, Y, test_size_range, random_state_range, max_depth_range):
    best_score = float('-inf')
    best_params = {}

    for test_size, random_state, max_depth in product(test_size_range, random_state_range, max_depth_range):
        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state)

        model_instance = model

        if hasattr(model_instance, 'random_state'):
            model_instance.set_params(random_state=random_state)
        if hasattr(model_instance, 'max_depth'):
            model_instance.set_params(max_depth=max_depth)

        model_instance.fit(X_train, Y_train)
        Y_pred = model_instance.predict(X_test)
        score = accuracy_score(Y_test, Y_pred)

        if score > best_score:
            best_score = score
            best_params = {'test_size': test_size, 'random_state': random_state, 'max_depth': max_depth}

    return {'name': name, 'best_params': best_params, 'best_score': best_score}

def find_best_params_regression(name, model, X, Y, test_size_range, random_state_range):
    best_score = float('inf')
    best_params = {}

    for test_size, random_state in product(test_size_range, random_state_range):
        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state)

        model_instance = model

        if hasattr(model_instance, 'random_state'):
            model_instance.set_params(random_state=random_state)

        model_instance.fit(X_train, Y_train)
        Y_pred = model_instance.predict(X_test)
        score = mean_squared_error(Y_test, Y_pred)

        if score < best_score:
            best_score = score
            best_params = {'test_size': test_size, 'random_state': random_state}

    return {'name': name, 'best_params': best_params, 'best_score': best_score}

def automl(df):
    try:
        if df is None:
            st.error('Please upload a dataset. If your dataset is prepared for training and testing, go to the next step.', icon="ğŸš¨")
            return      
        with st.container(border=True):
            st.info("Ä°ÅŸleminiz bir sÃ¼re devam edecek. Haydi baÅŸlayalÄ±m!", icon='ğŸ‰')
            df, label = data_preprocessing(df)

            # Model seÃ§enekleri ve kÄ±saltmalarÄ±
            classification_models = [
                ("LR", LogisticRegression()),
                ("LDA", LinearDiscriminantAnalysis()),
                ("KNN", KNeighborsClassifier()),
                ("DT", DecisionTreeClassifier()),
                ("NB", GaussianNB()),
                ("SVM", SVC()),
                ("RF", RandomForestClassifier()),
                ("GB", GradientBoostingClassifier()),
                ("XGB", XGBClassifier()),
                ("LGBM", LGBMClassifier()),
                ("CatBoost", CatBoostClassifier()),
                ("MLP", MLPClassifier()),
                ("AdaBoost", AdaBoostClassifier()),
                ("Bagging", BaggingClassifier()),
                ("ExtraTrees", ExtraTreesClassifier()),
                ("GaussianProcess", GaussianProcessClassifier())
            ]
            regression_models = [
                ("LinearRegression", LinearRegression()),
                ("Ridge", Ridge()),
                ("Lasso", Lasso()),
                ("ElasticNet", ElasticNet())
            ]

            # Hedef deÄŸiÅŸkeni seÃ§imi ve problem tÃ¼rÃ¼ seÃ§imi
            problem_type = st.selectbox("Problemin tÃ¼rÃ¼nÃ¼ seÃ§in:", options=['SÄ±nÄ±flandÄ±rma', 'Regresyon'])
            hedef_degisken = st.selectbox("Hedef DeÄŸiÅŸkeni SeÃ§in", df.columns.tolist())
            #st.write(problem_type)

            if hedef_degisken in df.columns:
                X = df.drop(hedef_degisken, axis=1)  # BaÄŸÄ±msÄ±z deÄŸiÅŸkenler
                Y = df[hedef_degisken]  # Hedef deÄŸiÅŸken
            else:
                st.write("Hedef deÄŸiÅŸken adÄ± geÃ§ersiz. LÃ¼tfen mevcut bir hedef deÄŸiÅŸken adÄ± girin.")
                return
            
            if problem_type == "SÄ±nÄ±flandÄ±rma":
                all_models = [name for name, _ in classification_models]
                first_model = st.selectbox("Ä°lk modeli seÃ§in:", all_models, index=0)
                second_model = st.selectbox("Ä°kinci modeli seÃ§in:", all_models, index=0)
                selected_models = [first_model, second_model]
                
            else:
                selected_models = ['LinearRegression', 'Ridge', 'Lasso', 'ElasticNet']

            ml_start = st.button("Makine Ã–ÄŸrenmesini BaÅŸlat.")
            
            if ml_start:
                st.info("Makine Ã¶ÄŸrenmesi baÅŸlatÄ±ldÄ±. LÃ¼tfen sonuÃ§larÄ± bekleyiniz.", icon='ğŸ¤–')

                if problem_type == 'SÄ±nÄ±flandÄ±rma':
                    st.info("SÄ±nÄ±flandÄ±rma yapÄ±lÄ±yor.", icon="â„¹ï¸")
                    models = [model for name, model in classification_models if name in selected_models]
                    find_best_params = find_best_params_classification
                    st.info("SÄ±nÄ±flandÄ±rma best parametreleri bulundu.", icon="â„¹ï¸")
                    
                    test_size_range = [0.1, 0.2, 0.3, 0.4]
                    random_state_range = [42, 2021, 12345]
                    max_depth_range = [3, 5, 7, 9]
                
                elif problem_type == 'Regresyon':
                    st.info("Regresyon yapÄ±lÄ±yor.", icon="â„¹ï¸")
                    models = [model for name, model in regression_models if name in selected_models]
                    find_best_params = find_best_params_regression
                    st.info("Regresyon best parametreleri bulundu. Bekleyiniz.", icon="â„¹ï¸")
                    
                    test_size_range = [0.1, 0.2, 0.3, 0.4]
                    random_state_range = [42, 2021, 12345]
                    max_depth_range = []
                
                futures = []
                if problem_type == 'SÄ±nÄ±flandÄ±rma':
                    with ProcessPoolExecutor() as executor:
                        for name, alg in classification_models:
                            if name in selected_models:
                                futures.append(
                                    executor.submit(find_best_params, name, alg, X, Y, test_size_range, random_state_range, max_depth_range))
                elif problem_type == 'Regresyon':
                    with ProcessPoolExecutor() as executor:
                        for name, alg in regression_models:
                            if name in selected_models:
                                futures.append(
                                    executor.submit(find_best_params, name, alg, X, Y, test_size_range, random_state_range))
                
                results = []
                for future in as_completed(futures):
                    result = future.result()
                    results.append(result)

                df_models = pd.DataFrame({
                    "Models": selected_models,
                    })
                
                # SonuÃ§larÄ± DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼rme
                df_results = pd.DataFrame(results)

                #    Streamlit arayÃ¼zÃ¼nde tabloyu gÃ¶sterme
                st.write("SeÃ§ilen Modeller")
                st.write(df_models)
                st.write("SonuÃ§lar")
                st.write(df_results)
       
    except Exception as e:
        st.write(f"Hata: {e}")

#MANUALML FONKSÄ°YONLARI
def save_model(model, format_choice, filename):
    if format_choice == 'joblib':
        joblib.dump(model, filename)
        st.write(f"Model baÅŸarÄ±yla '{filename}' adlÄ± dosyaya kaydedildi.")
    elif format_choice == 'pickle':
        with open(filename, 'wb') as f:
            pickle.dump(model, f)
        st.write(f"Model baÅŸarÄ±yla '{filename}' adlÄ± dosyaya kaydedildi.")
    elif format_choice == 'onnx':
        st.write("Bu model ONNX formatÄ±nda kaydedilemez.")
    else:
        st.write("GeÃ§ersiz format seÃ§imi! LÃ¼tfen 'joblib', 'pickle', 'h5', 'onnx', 'json' veya 'yaml' ÅŸeklinde bir format seÃ§in.")
def get_input_value(key):
    if key not in st.session_state:
        st.session_state[key] = ""
    return st.session_state[key]


def predict_new_data(df, model, columns, label_encoder):
    new_data = {}
    row_values = df.iloc[5]
    st.write(row_values)
    
    for column, value in zip(df.columns, row_values):
        new_data[column] = value

    print(new_data)
    
    new_df = pd.DataFrame(new_data, index=[0])

    # Yeni verilerin dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesi
    string_columns = new_df.select_dtypes(include=['object']).columns
    new_df[string_columns] = new_df[string_columns].apply(label_encoder.fit_transform)
    new_df = new_df.reindex(columns=columns, fill_value=0)

    st.write("OluÅŸturulan SÃ¶zlÃ¼k:")
    st.write(new_data)

    prediction = model.predict(new_df)
    st.write("Tahmin edilen hedef deÄŸiÅŸken:", prediction)

# KullanÄ±labilir modeller ve bunlarÄ±n adlarÄ±
models = [
        ("LR", LogisticRegression),
        ("LIR", LinearRegression),
        ("LDA", LinearDiscriminantAnalysis),
        ("KNN", KNeighborsClassifier),
        ("DT", DecisionTreeClassifier),
        ("NB", GaussianNB),
        ("SVM", SVC),
        ("RF", RandomForestClassifier),
        ("GB", GradientBoostingClassifier),
        ("XGB", XGBClassifier),
        ("LGBM", LGBMClassifier),
        ("CatBoost", CatBoostClassifier),
        ("MLP", MLPClassifier),
        ("AdaBoost", AdaBoostClassifier),
        ("Bagging", BaggingClassifier),
        ("ExtraTrees", ExtraTreesClassifier),
        ("GaussianProcess", GaussianProcessClassifier),
        ("Ridge Regression", Ridge),
        ("Lasso Regression", Lasso),
        ("ElasticNet Regresyon", ElasticNet)
    ]


def manualml(df):
    if df is None:
        st.error('Please upload a dataset. If your dataset is prepared for training and testing, go to the next step.', icon="ğŸš¨")
        return
    
    st.info("Ä°ÅŸleminiz bir sÃ¼re devam edecek. Haydi baÅŸlayalÄ±m!", icon='ğŸ‰')
     # data_preprocessing fonksiyonu tanÄ±mlanmalÄ± veya bu satÄ±r kaldÄ±rÄ±lmalÄ±
    # Veri setini uygun hale getirme
    df.drop_duplicates(inplace=True)
    df.columns = [kolon.lower() for kolon in df.columns]

    # Label Encoding iÅŸlemi
    label_encoder = LabelEncoder()
    string_columns = df.select_dtypes(include=['object']).columns
    df[string_columns] = df[string_columns].apply(label_encoder.fit_transform)
    st.write("Label Encoding iÅŸlemi tamamlandÄ±.")

    # Eksik deÄŸerleri doldurma
    integer_columns = df.select_dtypes(include=['int', 'float']).columns
    df[integer_columns] = df[integer_columns].fillna(df[integer_columns].mean())

    string_columns = df.select_dtypes(include=['object']).columns
    df[string_columns] = df[string_columns].fillna("Bilinmiyor")
    st.write("Eksik deÄŸerler dolduruldu.")


    # X ve y ayrÄ±ÅŸtÄ±rma
    target_variable = st.selectbox("Hedef deÄŸiÅŸkeni seÃ§in:", df.columns)
    X = df.drop(columns=[target_variable])
    y = df[target_variable]

    # KullanÄ±cÄ±dan test_size ve random_state deÄŸerlerini al
    # KullanÄ±cÄ±dan model seÃ§mesini iste
    
    test_size = st.number_input("Test setinin oranÄ±nÄ± girin (Ã¶rn: 0.2):", min_value=0.1, max_value=0.9, step=0.1, value=0.2)
    random_state = st.number_input("Random state deÄŸerini girin (Ã¶rn: 42):", min_value=30, step=2, value=30)

    all_models = [name for name, _ in models]
    selected_model_name = st.selectbox("Model SeÃ§in: ", all_models)
    selected_model_class = [model for name, model in models if name in selected_model_name]

    
    if selected_model_name in ['LR', 'LIR','SVM', 'MLP', 'GaussianProcess', 'Ridge Regression,', 'Lasso Regression', 'ElasticNet Regresyon']:
        pass
    elif selected_model_name in ["LDA", "KNNÄ°" "NB", "CatBoost"]:
        pass
    elif selected_model_name == 'DT':
        max_depth = st.number_input("max_depth oranÄ±nÄ± girin (Ã¶rn: 0.2):", min_value=1, max_value=50, step=2, value=1)

    elif selected_model_name in ['RF', 'GB', 'XGB', 'LGBM', 'ExtraTrees']:
        max_depth = st.number_input("max_depth oranÄ±nÄ± girin (Ã¶rn: 0.2):", min_value=1, max_value=50, step=2, value=1)
        n_estimators = st.number_input("n_estimators oranÄ±nÄ± girin (Ã¶rn: 0.2):", min_value=10, max_value=100, step=5, value=10)

    elif selected_model_name in ['Bagging', 'AdaBoost']:
        n_estimators = st.number_input("n_estimators oranÄ±nÄ± girin (Ã¶rn: 0.2):", min_value=10, max_value=100, step=5, value=10)
    else: 
        pass

    trainModel = st.button("Modeli EÄŸit.")

    if trainModel:
        st.info('Makine Ã¶ÄŸrenmesi baÅŸlamÄ±ÅŸtÄ±r, LÃ¼tfen bekleyiniz.', icon="â„¹ï¸")

        selected_model_class = selected_model_class[0]

        # SeÃ§ilen modelin parametrelerini kullanÄ±cÄ±ya gÃ¶ster
        params = {}
        st.write(f"SeÃ§ilen model: {selected_model_name}")
        st.write("Bu modelin alabileceÄŸi parametreler:")

        # Model parametrelerini kullanÄ±cÄ±dan al
        if 'n_estimators' in selected_model_class().get_params():
            params['n_estimators'] = n_estimators
        if 'random_state' in selected_model_class().get_params():
            params['random_state'] = random_state
        if 'max_depth' in selected_model_class().get_params():
            params['max_depth'] = max_depth

        st.write(params)

        # Modeli parametrelerle oluÅŸtur
        model = selected_model_class(**params)
        st.write(model)

        # Veriyi eÄŸitim ve test setine ayÄ±r
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)

        # Modeli eÄŸit ve test et
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        # Modelin parametrelerini yazdÄ±rma
        # st.write("Model Parametreleri:", model.get_params())

        # SonuÃ§larÄ± deÄŸerlendir
        if selected_model_name in ["LIR", "Ridge Regression", "Lasso Regression", "ElasticNet Regresyon"]:
            # Regresyon modelleri iÃ§in
            mse = mean_squared_error(y_test, y_pred)
            st.write("Mean Squared Error:", mse)
        else:
            # SÄ±nÄ±flandÄ±rma modelleri iÃ§in
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted')
            recall = recall_score(y_test, y_pred, average='weighted')
            f1 = f1_score(y_test, y_pred, average='weighted')

            # SonuÃ§larÄ± yazdÄ±rma
            st.write('*SonuÃ§lar*')
            st.write("Accuracy:", accuracy)
            st.write("Precision:", precision)
            st.write("Recall:", recall)
            st.write("F1 Score:", f1)

            # Yeni veri ile tahmin yapma
        predict_new_data(df, model, X.columns, label_encoder)
            
        #filename = input("Kaydetmek istediÄŸiniz dosyanÄ±n adÄ±nÄ± girin (Ã¶rn: model.pkl, model.h5, model.onnx, model.json, model.yaml): ")
        #save_model(model, format_choice, filename)   

# FRONTEND FONKSÄ°YONU
def frontend():
    st.subheader("Machine Learning")
    st.write("Machine learning is a subfield of artificial intelligence that enables computers to learn from data and make predictions without being explicitly programmed. In this process, computers learn patterns and relationships based on data and make predictions that apply to future data. Machine learning draws meaningful conclusions from data using various algorithms and techniques. The basic components of machine learning are data, model and algorithm. The data contains the examples used for the model to learn. The model is a mathematical representation learned from the data. The algorithm is the method used to train the model. Machine learning is used in a wide variety of fields. Prediction and classification are among the most common uses. Machine learning is a powerful tool for analyzing large amounts of data and making decisions based on that data. This provides great advantages in many fields, from business to healthcare. Through machine learning, more accurate predictions can be made, efficiency can be increased and new discoveries can be made.")
    with st.expander("ğŸ”— *Important Terms for Machine Learning*"):
        st.write("Target Variable:: The variable that the model is trying to predict. It is also called the dependent variable in supervised learning. Example: If we want to predict the price of a sample car in a car data set, the target variable is the price.")
        st.write("Random State: A constant value used for random processes. It is used to ensure repeatability of the model and consistency of results. Example: Random state is used to get the same result when randomly dividing the training and test data.")
        st.write("Test Size: Determines the proportion of the test set when dividing the data into training and test sets. It is usually specified as a ratio between 0 and 1. Example: test_size=0.2 indicates that 20% of the data will be used as test set and 80% as training set.")
        st.write("Best Score: The best success metric used to evaluate the performance of the model.)It can refer to the accuracy of the model, accuracy rate, accuracy score performance metric. Example: The greater the accuracy rate best_score of a model, the better the best score.")
    
    st.info('If you want to know more in detail, read our [**manual**](https://docs.google.com/document/d/1sGUF0wu6fIFcyUCQO1Ir_rTdPcdQdK-rz6pm9lkrrxM/edit?usp=sharing).', icon="â„¹ï¸")
   
    # Dosya yÃ¼kleme iÅŸlemi iÃ§in Streamlit'in file_uploader fonksiyonunu kullanma
    uploaded_file = st.file_uploader("Upload a file", type=["csv", "json"])

    ml_option = st.selectbox("Do you want to do your training yourself or should it be done automatically?", ["Choose", "I'll do it myself.", "Do it."])

    if uploaded_file is None:
        st.error('Please upload a dataset. If your dataset is prepared for training and testing, go to the next step.', icon="ğŸš¨")
    elif ml_option == "I'll do it myself.":
        df = handle_file_upload(uploaded_file)
        manualml(df)
    elif ml_option == "Do it.":
        df = handle_file_upload(uploaded_file)
        automl(df)
    else:
        st.write("Please made an valid selection.")

if __name__ == "__main__":
    # Page configuration
    st.set_page_config(page_title="ATOM AI", layout="wide",)
    # YakÄ±nsama uyarÄ±larÄ±nÄ± bastÄ±rmak iÃ§in ayarlarÄ± yapÄ±landÄ±rma
    filterwarnings("ignore", category=ConvergenceWarning)

    #Frontend ile arayÃ¼zÃ¼n oluÅŸturulmasÄ±.
    frontend()
